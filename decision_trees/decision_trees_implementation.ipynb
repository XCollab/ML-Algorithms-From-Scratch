{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Decision Tree from Scratch:<br> A Comprehensive Guide  </center>\n",
    "\n",
    "## üåê **Follow Me on Social Media:**  \n",
    "\n",
    "- [![LinkedIn](https://img.shields.io/badge/LinkedIn-Connect-blue?style=flat&logo=linkedin)](https://linkedin.com/in/codewithdark)  \n",
    "- [![GitHub](https://img.shields.io/badge/GitHub-Follow-black?style=flat&logo=github)](https://github.com/codewithdark-git)  \n",
    "- [![Kaggle](https://img.shields.io/badge/Kaggle-Follow-blue?style=flat&logo=kaggle)](https://kaggle.com/codewithdark)\n",
    "- [![See Notebook](https://img.shields.io/badge/colab-View%20Notebooks-blue?style=flat&logo=colab)](https://github.com/codewithdark-git/ML-Algorithms-From-Scratch/blob/88da3d72945397d913a1cafbc8d4280bd80538c7/decision_trees/decision_trees_implementation.ipynb)\n",
    "- [![See Blog Post](https://img.shields.io/badge/Read-Blog%20Post-blue?style=flat&logo=readme)](https://gist.github.com/codewithdark-git)\n",
    "\n",
    "\n",
    "Decision trees are one of the most popular and interpretable algorithms in machine learning, commonly used for both classification and regression tasks. They work by recursively splitting the dataset based on feature thresholds, creating a tree-like structure where each internal node represents a decision based on a feature, and each leaf node corresponds to an output label or value. \n",
    "\n",
    "The main advantages of decision trees are their simplicity, ease of visualization, and ability to handle both numerical and categorical data. However, when implemented from scratch, careful handling of split criteria, stopping conditions, and data preprocessing is required to ensure the model performs optimally.\n",
    "\n",
    "This document provides an in-depth exploration of implementing a decision tree from scratch in Python. It covers key concepts such as splitting data, calculating information gain using metrics like Gini Impurity and Entropy, and building the tree structure recursively. Additionally, the implementation accounts for various parameters like maximum tree depth and minimum samples required for a split to prevent overfitting.\n",
    "\n",
    "By understanding and building a decision tree from the ground up, we gain valuable insights into the mechanics of tree-based algorithms and lay a strong foundation for extending these concepts to advanced methods like Random Forests and Gradient Boosted Trees.\n",
    "\n",
    "![](Images/_-%20visual%20selection%20(1).png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Gini Impurity Formula**\n",
    "\n",
    "$$\n",
    "Gini(y) = 1 - \\sum_{i=1}^{k} p_i^2\n",
    "$$\n",
    "\n",
    "- **Explanation**:\n",
    "  - $( p_i )$ is the proportion of class $( i )$ in the dataset.\n",
    "  - $( k )$ is the total number of classes.\n",
    "\n",
    "  - `np.unique(y, return_counts=True)` calculates the unique classes and their counts.\n",
    "  - `probabilities = counts / len(y)` computes $( p_i )$ for each class.\n",
    "  - `1 - np.sum(probabilities**2)` computes $( Gini(y) )$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_impurity(y):\n",
    "    classes, counts = np.unique(y, return_counts=True)\n",
    "    probabilities = counts / len(y)\n",
    "    return 1 - np.sum(probabilities**2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **2. Entropy Formula**\n",
    "\n",
    "$$\n",
    "Entropy(y) = - \\sum_{i=1}^{k} p_i \\log_2(p_i)\n",
    "$$\n",
    "\n",
    "- **Explanation**:\n",
    "  - $( p_i )$ is the proportion of class $( i )$ in the dataset.\n",
    "  - Entropy measures the \"impurity\" or \"uncertainty\" in the dataset.\n",
    "\n",
    "  - $( \\log_2(p_i) )$ is calculated using `np.log2(probabilities + 1e-9)`.\n",
    "  - Adding $( 1e-9 )$ prevents numerical errors when $( p_i = 0 )$.\n",
    "\n",
    "![](Images/_-%20visual%20selection%20(2).png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(y):\n",
    "    classes, counts = np.unique(y, return_counts=True)\n",
    "    probabilities = counts / len(y)\n",
    "    return -np.sum(probabilities * np.log2(probabilities + 1e-9))  # Add small value to avoid log(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X, y, feature_index, threshold):\n",
    "    left_indices = X[:, feature_index] <= threshold\n",
    "    right_indices = X[:, feature_index] > threshold\n",
    "    return X[left_indices], X[right_indices], y[left_indices], y[right_indices]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Information Gain Formula**\n",
    "\n",
    "\n",
    "$$\n",
    "Gain = Impurity_{parent} - \\left( \\frac{n_{left}}{n} \\cdot Impurity_{left} + \\frac{n_{right}}{n} \\cdot Impurity_{right} \\right)\n",
    "$$\n",
    "\n",
    "- **Explanation**:\n",
    "  - $( Impurity_{parent})$: Gini or Entropy of the parent node.\n",
    "  - $( Impurity_{left})$: Gini or Entropy of the left child node.\n",
    "  - $( Impurity_{right})$: Gini or Entropy of the right child node.\n",
    "  - $( n_{left}, n_{right})$: Number of samples in the left and right child nodes.\n",
    "  - $( n )$: Total number of samples in the parent node.\n",
    "\n",
    "  - The parent impurity is calculated as `impurity_function(y)`.\n",
    "  - Weighted child impurity is calculated using the proportions $( \\frac{n_{left}}{n} )$ and $( \\frac{n_{right}}{n} )$.\n",
    "\n",
    "\n",
    "![](Images/_-%20visual%20selection%20(3).png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_gain(y, y_left, y_right, impurity_function=gini_impurity):\n",
    "    parent_impurity = impurity_function(y)\n",
    "    n = len(y)\n",
    "    n_left, n_right = len(y_left), len(y_right)\n",
    "    \n",
    "    # Weighted impurity of children\n",
    "    child_impurity = (n_left / n) * impurity_function(y_left) + (n_right / n) * impurity_function(y_right)\n",
    "    \n",
    "    return parent_impurity - child_impurity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Node` class represents a single node in the decision tree. Each node can either be:\n",
    "\n",
    "1. **An internal node**: Contains information about a feature index and threshold used for splitting the data, along with pointers to its left and right child nodes.\n",
    "2. **A leaf node**: Contains a classification value when further splits are no longer possible or desirable.\n",
    "\n",
    "Here‚Äôs an explanation of each parameter in the `Node` class:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. `feature_index`**\n",
    "- **Purpose**:\n",
    "  - Stores the index of the feature used for splitting at this node.\n",
    "  - Example: If `feature_index = 2`, it means this node splits based on the third feature in the dataset.\n",
    "\n",
    "- **Type**: Integer or `None`.\n",
    "- **Usage**:\n",
    "  - Used only in internal nodes. It is `None` for leaf nodes.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. `threshold`**\n",
    "- **Purpose**:\n",
    "  - Stores the threshold value for the feature used to split the data at this node.\n",
    "  - Example: If `threshold = 5.5`, this node splits data into:\n",
    "    - Left child: Samples where the feature value is ‚â§ 5.5.\n",
    "    - Right child: Samples where the feature value is > 5.5.\n",
    "\n",
    "- **Type**: Float or `None`.\n",
    "- **Usage**:\n",
    "  - Used only in internal nodes. It is `None` for leaf nodes.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. `left`**\n",
    "- **Purpose**:\n",
    "  - A reference to the left child node.\n",
    "  - Represents the subset of data that satisfies the condition `feature_value <= threshold`.\n",
    "\n",
    "- **Type**: Instance of `Node` or `None`.\n",
    "- **Usage**:\n",
    "  - Points to the left child in the decision tree structure.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. `right`**\n",
    "- **Purpose**:\n",
    "  - A reference to the right child node.\n",
    "  - Represents the subset of data that satisfies the condition `feature_value > threshold`.\n",
    "\n",
    "- **Type**: Instance of `Node` or `None`.\n",
    "- **Usage**:\n",
    "  - Points to the right child in the decision tree structure.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. `value`**\n",
    "- **Purpose**:\n",
    "  - Stores the value of the prediction (or class label) at a **leaf node**.\n",
    "  - For classification tasks:\n",
    "    - It‚Äôs the most common label in the data at this node.\n",
    "  - For regression tasks:\n",
    "    - It‚Äôs the mean or another aggregation metric of the target values at this node.\n",
    "\n",
    "- **Type**: Depends on the task:\n",
    "  - For classification: Integer (class label).\n",
    "  - For regression: Float (predicted value).\n",
    "  - It is `None` for internal nodes.\n",
    "\n",
    "---\n",
    "\n",
    "### **Node Behavior**\n",
    "- **Internal Nodes**:\n",
    "  - Contain `feature_index`, `threshold`, `left`, and `right`.\n",
    "  - Example:\n",
    "    ```python\n",
    "    Node(feature_index=1, threshold=2.5, left=left_node, right=right_node)\n",
    "    ```\n",
    "\n",
    "- **Leaf Nodes**:\n",
    "  - Contain `value` and no references to children.\n",
    "  - Example:\n",
    "    ```python\n",
    "    Node(value=0)\n",
    "    ```\n",
    "\n",
    "---\n",
    "\n",
    "### **Example Usage**\n",
    "\n",
    "#### **Internal Node Example**:\n",
    "An internal node that splits based on the feature at index 2 with a threshold of 3.5:\n",
    "```python\n",
    "node = Node(feature_index=2, threshold=3.5, left=left_child, right=right_child)\n",
    "```\n",
    "\n",
    "#### **Leaf Node Example**:\n",
    "A leaf node that predicts class `1`:\n",
    "```python\n",
    "leaf = Node(value=1)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Integration with `DecisionTree`**\n",
    "\n",
    "- When building the tree (`_build_tree` method):\n",
    "  - Internal nodes are created with `feature_index`, `threshold`, and pointers to child nodes.\n",
    "  - Leaf nodes are created with `value` when the stopping criteria are met.\n",
    "\n",
    "![](Images/###%20Introduction%20to%20Decision%20Trees%20-%20visual%20selection%20(1).png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, feature_index=None, threshold=None, left=None, right=None, value=None):\n",
    "        self.feature_index = feature_index  # Index of feature to split\n",
    "        self.threshold = threshold          # Threshold for splitting\n",
    "        self.left = left                    # Left child\n",
    "        self.right = right                  # Right child\n",
    "        self.value = value                  # Leaf node value (for classification)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. `__init__(self, max_depth=5, min_samples_split=2)`**\n",
    "\n",
    "- **Purpose**:\n",
    "  Initializes the decision tree with two hyperparameters:\n",
    "  - `max_depth`: The maximum depth the tree is allowed to grow.\n",
    "  - `min_samples_split`: The minimum number of samples required to split a node.\n",
    "\n",
    "- **Attributes**:\n",
    "  - `self.root`: The root node of the decision tree, initialized as `None`.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. `_build_tree(self, X, y, depth)`**\n",
    "\n",
    "- **Purpose**:\n",
    "  Recursively builds the decision tree.\n",
    "\n",
    "- **Steps**:\n",
    "  1. **Check Stopping Criteria**:\n",
    "     - Stops growing the tree if:\n",
    "       - Maximum depth is reached.\n",
    "       - Number of samples is less than `min_samples_split`.\n",
    "       - All samples belong to the same class.\n",
    "\n",
    "  2. **Find the Best Split**:\n",
    "     - Loops over all features and possible thresholds.\n",
    "     - Uses `information_gain` to evaluate each split.\n",
    "     - Tracks the best feature and threshold.\n",
    "\n",
    "  3. **No Valid Split**:\n",
    "     - If no split improves the gain, create a **leaf node** with the most common label in `y`.\n",
    "\n",
    "  4. **Split Data and Recur**:\n",
    "     - Splits data into `X_left` and `X_right`.\n",
    "     - Recursively calls `_build_tree` to build the left and right children.\n",
    "\n",
    "- **Returns**:\n",
    "  A `Node` object (either a leaf node or an internal node).\n",
    "\n",
    "---\n",
    "\n",
    "### **3. `_most_common_label(self, y)`**\n",
    "\n",
    "- **Purpose**:\n",
    "  Finds the most common class in the given labels `y`.\n",
    "\n",
    "- **Steps**:\n",
    "  - Uses `np.unique` to count occurrences of each class.\n",
    "  - Returns the class with the highest count using `np.argmax`.\n",
    "\n",
    "- **Returns**:\n",
    "  The majority class in `y`.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. `fit(self, X, y)`**\n",
    "\n",
    "- **Purpose**:\n",
    "  Fits (or trains) the decision tree on the training data.\n",
    "\n",
    "- **Steps**:\n",
    "  - Calls `_build_tree` with the training data `X`, labels `y`, and an initial depth of `0`.\n",
    "  - Stores the resulting tree in `self.root`.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. `_predict(self, x, node)`**\n",
    "\n",
    "- **Purpose**:\n",
    "  Predicts the class for a single sample `x` by traversing the tree.\n",
    "\n",
    "- **Steps**:\n",
    "  - If the current `node` is a **leaf node**, return its value.\n",
    "  - If `x[node.feature_index] <= node.threshold`, recurse into the left child.\n",
    "  - Otherwise, recurse into the right child.\n",
    "\n",
    "- **Returns**:\n",
    "  The predicted class for the input sample `x`.\n",
    "\n",
    "---\n",
    "\n",
    "### **6. `predict(self, X)`**\n",
    "\n",
    "- **Purpose**:\n",
    "  Predicts the class for all samples in the dataset `X`.\n",
    "\n",
    "- **Steps**:\n",
    "  - Loops through each sample in `X` and calls `_predict` for it.\n",
    "  - Collects predictions into a NumPy array.\n",
    "\n",
    "- **Returns**:\n",
    "  A NumPy array of predictions for all samples.\n",
    "\n",
    "---\n",
    "\n",
    "### **7. `_count_nodes(self, node, counts)`**\n",
    "\n",
    "- **Purpose**:\n",
    "  Recursively counts the number of nodes in the tree, including:\n",
    "  - Root node\n",
    "  - Internal nodes\n",
    "  - Leaf nodes\n",
    "\n",
    "- **Steps**:\n",
    "  - If the node is a **leaf node** (its value is not `None`), increment the `leaves` count.\n",
    "  - Otherwise, increment the `internal_nodes` count.\n",
    "  - Recurse into the left and right children.\n",
    "\n",
    "---\n",
    "\n",
    "### **8. `count_nodes(self)`**\n",
    "\n",
    "- **Purpose**:\n",
    "  Provides a summary of the number of different types of nodes in the tree.\n",
    "\n",
    "- **Steps**:\n",
    "  - Initializes a dictionary `counts` with:\n",
    "    - `root`: 1 (always one root).\n",
    "    - `internal_nodes`: 0.\n",
    "    - `leaves`: 0.\n",
    "  - Calls `_count_nodes` starting from the root node.\n",
    "\n",
    "- **Returns**:\n",
    "  A dictionary containing the counts of root, internal, and leaf nodes.\n",
    "\n",
    "---\n",
    "\n",
    "### **9. `_print_tree(self, node, depth=0)`**\n",
    "\n",
    "- **Purpose**:\n",
    "  Recursively prints the structure of the decision tree.\n",
    "\n",
    "- **Steps**:\n",
    "  - For **leaf nodes**, print \"Leaf Node: Class = ...\" with indentation proportional to depth.\n",
    "  - For **internal nodes**, print \"Internal Node: Feature[...] <= ...\" with the feature and threshold.\n",
    "  - Recurse into the left and right children, increasing the depth.\n",
    "\n",
    "---\n",
    "\n",
    "### **10. `print_tree(self)`**\n",
    "\n",
    "- **Purpose**:\n",
    "  Prints the entire tree structure starting from the root.\n",
    "\n",
    "- **Steps**:\n",
    "  - Calls `_print_tree` with the root node and an initial depth of `0`.\n",
    "\n",
    "---\n",
    "\n",
    "### **Class Summary**\n",
    "\n",
    "This `DecisionTree` class:\n",
    "1. **Builds a Tree**:\n",
    "   - Using recursive splitting based on the best information gain.\n",
    "2. **Predicts Classes**:\n",
    "   - Traverses the tree to make predictions for given inputs.\n",
    "3. **Analyzes the Tree**:\n",
    "   - Counts the types of nodes.\n",
    "   - Prints the tree structure.\n",
    "  \n",
    "\n",
    "![](Images/class%20DecisionTree_%20-%20visual%20selection%20(1).png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=5, min_samples_split=2):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.root = None\n",
    "\n",
    "    def _build_tree(self, X, y, depth):\n",
    "        n_samples, n_features = X.shape\n",
    "        unique_classes = np.unique(y)\n",
    "\n",
    "        # Stop criteria\n",
    "        if depth >= self.max_depth or n_samples < self.min_samples_split or len(unique_classes) == 1:\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return Node(value=leaf_value)\n",
    "\n",
    "        # Find the best split\n",
    "        best_gain = -1\n",
    "        best_feature, best_threshold = None, None\n",
    "\n",
    "        for feature_index in range(n_features):\n",
    "            thresholds = np.unique(X[:, feature_index])\n",
    "            for threshold in thresholds:\n",
    "                X_left, X_right, y_left, y_right = split_data(X, y, feature_index, threshold)\n",
    "                if len(y_left) > 0 and len(y_right) > 0:\n",
    "                    gain = information_gain(y, y_left, y_right)\n",
    "                    if gain > best_gain:\n",
    "                        best_gain, best_feature, best_threshold = gain, feature_index, threshold\n",
    "\n",
    "        # If no split improves the gain, create a leaf node\n",
    "        if best_gain == -1:\n",
    "            leaf_value = self._most_common_label(y)\n",
    "            return Node(value=leaf_value)\n",
    "\n",
    "        # Split the data\n",
    "        X_left, X_right, y_left, y_right = split_data(X, y, best_feature, best_threshold)\n",
    "\n",
    "        # Recursively build children\n",
    "        left_child = self._build_tree(X_left, y_left, depth + 1)\n",
    "        right_child = self._build_tree(X_right, y_right, depth + 1)\n",
    "\n",
    "        return Node(feature_index=best_feature, threshold=best_threshold, left=left_child, right=right_child)\n",
    "\n",
    "    def _most_common_label(self, y):\n",
    "        classes, counts = np.unique(y, return_counts=True)\n",
    "        return classes[np.argmax(counts)]\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Convert X and y to NumPy arrays if they are DataFrames or Series\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        self.root = self._build_tree(X, y, 0)\n",
    "\n",
    "\n",
    "    def _predict(self, x, node):\n",
    "        if node.value is not None:\n",
    "            return node.value\n",
    "\n",
    "        if x[node.feature_index] <= node.threshold:\n",
    "            return self._predict(x, node.left)\n",
    "        else:\n",
    "            return self._predict(x, node.right)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._predict(x, self.root) for x in X])\n",
    "\n",
    "    def _count_nodes(self, node, counts):\n",
    "        if node is None:\n",
    "            return\n",
    "\n",
    "        if node.value is not None:  # Leaf node\n",
    "            counts[\"leaves\"] += 1\n",
    "        else:  # Internal or root node\n",
    "            counts[\"internal_nodes\"] += 1\n",
    "\n",
    "        # Recursively count for left and right children\n",
    "        self._count_nodes(node.left, counts)\n",
    "        self._count_nodes(node.right, counts)\n",
    "\n",
    "    def count_nodes(self):\n",
    "        counts = {\"root\": 1, \"internal_nodes\": 0, \"leaves\": 0}\n",
    "        self._count_nodes(self.root, counts)\n",
    "        return counts\n",
    "\n",
    "    def _print_tree(self, node, depth=0):\n",
    "        if node is None:\n",
    "            return\n",
    "\n",
    "        if node.value is not None:  # Leaf node\n",
    "            print(f\"{'|   ' * depth}Leaf Node: Class = {node.value}\")\n",
    "        else:\n",
    "            print(f\"{'|   ' * depth}Internal Node: Feature[{node.feature_index}] <= {node.threshold}\")\n",
    "        \n",
    "        # Traverse left and right children\n",
    "        self._print_tree(node.left, depth + 1)\n",
    "        self._print_tree(node.right, depth + 1)\n",
    "\n",
    "    def print_tree(self):\n",
    "        print(\"Decision Tree Structure:\")\n",
    "        self._print_tree(self.root)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>125</td>\n",
       "      <td>212</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>168</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "      <td>203</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>155</td>\n",
       "      <td>1</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>145</td>\n",
       "      <td>174</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>125</td>\n",
       "      <td>1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>148</td>\n",
       "      <td>203</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>161</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>138</td>\n",
       "      <td>294</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>106</td>\n",
       "      <td>0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1020</th>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>140</td>\n",
       "      <td>221</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>164</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1021</th>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>125</td>\n",
       "      <td>258</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>141</td>\n",
       "      <td>1</td>\n",
       "      <td>2.8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1022</th>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>110</td>\n",
       "      <td>275</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>118</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1023</th>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>110</td>\n",
       "      <td>254</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>159</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1024</th>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>188</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>113</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1025 rows √ó 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  \\\n",
       "0      52    1   0       125   212    0        1      168      0      1.0   \n",
       "1      53    1   0       140   203    1        0      155      1      3.1   \n",
       "2      70    1   0       145   174    0        1      125      1      2.6   \n",
       "3      61    1   0       148   203    0        1      161      0      0.0   \n",
       "4      62    0   0       138   294    1        1      106      0      1.9   \n",
       "...   ...  ...  ..       ...   ...  ...      ...      ...    ...      ...   \n",
       "1020   59    1   1       140   221    0        1      164      1      0.0   \n",
       "1021   60    1   0       125   258    0        0      141      1      2.8   \n",
       "1022   47    1   0       110   275    0        0      118      1      1.0   \n",
       "1023   50    0   0       110   254    0        0      159      0      0.0   \n",
       "1024   54    1   0       120   188    0        1      113      0      1.4   \n",
       "\n",
       "      slope  ca  thal  target  \n",
       "0         2   2     3       0  \n",
       "1         0   0     3       0  \n",
       "2         0   0     3       0  \n",
       "3         2   1     3       0  \n",
       "4         1   3     2       0  \n",
       "...     ...  ..   ...     ...  \n",
       "1020      2   0     2       1  \n",
       "1021      1   1     3       0  \n",
       "1022      1   1     2       0  \n",
       "1023      2   0     2       1  \n",
       "1024      1   1     3       0  \n",
       "\n",
       "[1025 rows x 14 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "# data = load_iris()\n",
    "# X, y = data.data, data.target\n",
    "\n",
    "df = pd.read_csv('heart.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, 0:-1]\n",
    "y = df.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "for column in X_train.columns:\n",
    "    if X_train[column].dtype == 'object':  # Encode only non-numerical columns\n",
    "        X_train[column] = le.fit_transform(X_train[column])\n",
    "        X_test[column] = le.transform(X_test[column])\n",
    "\n",
    "# Convert to NumPy arrays for compatibility\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Images/_-%20visual%20selection.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following code shows the performance of the Build from Scratch tree algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Scratch Tree : 0.9853658536585366\n"
     ]
    }
   ],
   "source": [
    "# Train decision tree\n",
    "tree = DecisionTree(max_depth=10)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = tree.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy of Scratch Tree : {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following code shows the performance of the Sklearn tree algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of sklearn Tree: 0.9853658536585366\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree1 = DecisionTreeClassifier(max_depth=10)\n",
    "\n",
    "tree1.fit(X_train, y_train)\n",
    "\n",
    "predictions1 = tree1.predict(X_test)\n",
    "\n",
    "accuracy1 = accuracy_score(y_test, predictions1)\n",
    "print(f\"Accuracy of sklearn Tree: {accuracy1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Node: 1\n",
      "Internal Nodes: 2\n",
      "Leaf Nodes: 3\n"
     ]
    }
   ],
   "source": [
    "# Count the parameters of the tree\n",
    "counts = tree.count_nodes()\n",
    "print(f\"Root Node: 1\")\n",
    "print(f\"Internal Nodes: {counts['internal_nodes']}\")\n",
    "print(f\"Leaf Nodes: {counts['leaves']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Structure:\n",
      "Internal Node: Feature[2] <= 1.9\n",
      "|   Leaf Node: Class = 0\n",
      "|   Internal Node: Feature[2] <= 4.7\n",
      "|   |   Leaf Node: Class = 1\n",
      "|   |   Leaf Node: Class = 2\n"
     ]
    }
   ],
   "source": [
    "# Print the decision tree\n",
    "tree.print_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
