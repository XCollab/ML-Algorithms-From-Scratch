{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Implementation from Scratch\n",
    "\n",
    "This notebook implements Linear Regression from scratch using NumPy. We'll build the algorithm step by step and visualize the results.\n",
    "\n",
    "## What is Linear Regression?\n",
    "Linear regression is a fundamental algorithm in machine learning that models the relationship between a dependent variable (target) and one or more independent variables (features) by fitting a linear equation to the observed data.\n",
    "\n",
    "The equation takes the form: y = wx + b\n",
    "- y is the predicted value\n",
    "- w is the weight (or coefficient)\n",
    "- x is the feature value\n",
    "- b is the bias (or intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "\n",
    "We'll need:\n",
    "- NumPy for numerical computations\n",
    "- Matplotlib for data visualization\n",
    "- %matplotlib inline to display plots in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Sample Data\n",
    "\n",
    "We'll create synthetic data with a known relationship:\n",
    "- True relationship: y = 4 + 3x + noise\n",
    "- x values are randomly generated between 0 and 2\n",
    "- Gaussian noise is added to make it more realistic\n",
    "\n",
    "This way, we can verify if our model learns the correct parameters (w ≈ 3 and b ≈ 4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate random data points\n",
    "X = 2 * np.random.rand(100, 1)  # 100 random x values between 0 and 2\n",
    "y = 4 + 3 * X + np.random.randn(100, 1)  # True relationship with added noise\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X, y, color='blue', label='Data points')\n",
    "plt.xlabel('X (Input feature)')\n",
    "plt.ylabel('y (Target variable)')\n",
    "plt.title('Generated Data: y = 4 + 3x + noise')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Class Implementation\n",
    "\n",
    "Our implementation includes:\n",
    "1. Initialization with hyperparameters\n",
    "2. Gradient descent optimization\n",
    "3. Mean Squared Error (MSE) loss function\n",
    "4. Prediction method\n",
    "5. Loss history tracking for visualization\n",
    "\n",
    "### Key Concepts:\n",
    "- **Gradient Descent**: Iteratively updates parameters to minimize the loss function\n",
    "- **Learning Rate**: Controls how much we adjust parameters in each iteration\n",
    "- **Loss Function**: Measures how well our predictions match the true values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class LinearRegressionScratch:\n",
    "    def __init__(self, learning_rate=0.01, n_iterations=1000):\n",
    "        \"\"\"Initialize the model with hyperparameters\"\"\"\n",
    "        self.learning_rate = learning_rate  # Step size for gradient descent\n",
    "        self.n_iterations = n_iterations    # Number of training iterations\n",
    "        self.weights = None                # Model weights (w)\n",
    "        self.bias = None                   # Model bias (b)\n",
    "        self.history = {'loss': []}        # Track loss during training\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Train the model using gradient descent\"\"\"\n",
    "        # Initialize parameters\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "        \n",
    "        # Gradient descent\n",
    "        for _ in range(self.n_iterations):\n",
    "            # Forward pass: compute predictions\n",
    "            y_predicted = np.dot(X, self.weights) + self.bias\n",
    "            \n",
    "            # Compute gradients\n",
    "            dw = (1/n_samples) * np.dot(X.T, (y_predicted - y))  # derivative w.r.t weights\n",
    "            db = (1/n_samples) * np.sum(y_predicted - y)         # derivative w.r.t bias\n",
    "            \n",
    "            # Update parameters\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "            \n",
    "            # Track loss\n",
    "            loss = self._compute_loss(y, y_predicted)\n",
    "            self.history['loss'].append(loss)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions for given input X\"\"\"\n",
    "        return np.dot(X, self.weights) + self.bias\n",
    "    \n",
    "    def _compute_loss(self, y_true, y_predicted):\n",
    "        \"\"\"Compute Mean Squared Error loss\"\"\"\n",
    "        return np.mean((y_true - y_predicted) ** 2)\n",
    "    \n",
    "    def plot_loss_history(self):\n",
    "        \"\"\"Visualize how loss changes during training\"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(self.history['loss'])\n",
    "        plt.title('Loss History During Training')\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Mean Squared Error')\n",
    "        plt.grid(True)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "\n",
    "Now we'll:\n",
    "1. Create an instance of our LinearRegression class\n",
    "2. Train it on our generated data\n",
    "3. Visualize how the loss decreases during training\n",
    "\n",
    "The decreasing loss indicates that our model is learning the underlying pattern in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create and train the model\n",
    "model = LinearRegressionScratch(learning_rate=0.01, n_iterations=1000)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Plot loss history\n",
    "model.plot_loss_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate and Visualize Results\n",
    "\n",
    "Let's:\n",
    "1. Make predictions on our data\n",
    "2. Plot the original data points and our model's predictions\n",
    "3. Compare the learned parameters with the true values\n",
    "\n",
    "Remember, our true relationship was y = 4 + 3x + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Make predictions\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X, y, color='blue', label='Actual Data')\n",
    "plt.plot(X, y_pred, color='red', label='Model Predictions')\n",
    "plt.xlabel('X (Input feature)')\n",
    "plt.ylabel('y (Target variable)')\n",
    "plt.title('Linear Regression: Actual vs Predicted')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Print model parameters\n",
    "print(\"Model Parameters:\")\n",
    "print(f\"Weight (w): {model.weights[0]:.4f} (True value: 3)\")\n",
    "print(f\"Bias (b): {model.bias:.4f} (True value: 4)\")\n",
    "\n",
    "# Calculate and print R-squared score\n",
    "y_mean = np.mean(y)\n",
    "r2 = 1 - np.sum((y - y_pred)**2) / np.sum((y - y_mean)**2)\n",
    "print(f\"\\nR² Score: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Our implementation successfully:\n",
    "1. Learned the underlying relationship in the data\n",
    "2. Found parameters close to the true values (w ≈ 3, b ≈ 4)\n",
    "3. Made accurate predictions as shown by the visualization\n",
    "\n",
    "This demonstrates how linear regression works from the ground up, including:\n",
    "- Parameter initialization\n",
    "- Gradient descent optimization\n",
    "- Loss function computation\n",
    "- Model prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
